{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0120ENSkillsNetwork20629446-2022-01-01\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DL0120ENedX/labs/Template%20for%20Instructional%20Hands-on%20Labs/images/IDSNlogo.png\" width=\"400px\" align=\"center\"></a>\n",
    "\n",
    "<h1 align=\"center\"><font size=\"5\">AUTOENCODERS</font></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **25** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "Welcome to this notebook about autoencoders.\n",
    "<font size=\"3\"><strong>In this notebook you will learn the definition of an autoencoder, how it works, and see an implementation in TensorFlow.</strong></font>\n",
    "<br>\n",
    "<br>\n",
    "<h2>Table of Contents</h2>\n",
    "<ol>\n",
    " <li><a href=\"https://#ref1\">Introduction</a></li>\n",
    " <li><a href=\"https://#ref2\">Feature Extraction and Dimensionality Reduction</a></li>\n",
    " <li><a href=\"https://#ref3\">Autoencoder Structure</a></li>\n",
    " <li><a href=\"https://#ref4\">Performance</a></li>\n",
    " <li><a href=\"https://#ref5\">Training: Loss Function</a></li>\n",
    " <li><a href=\"https://#ref6\">Code</a></li>\n",
    "</ol>\n",
    "</div>\n",
    "<br>\n",
    "By the end of this notebook, you should be able to create simple autoencoders apply them to problems in the field of unsupervised learning.\n",
    "<br>\n",
    "<p></p>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref1\"></a>\n",
    "\n",
    "<h2>Introduction</h2>\n",
    "An autoencoder, also known as autoassociator or Diabolo networks, is an artificial neural network employed to recreate the given input.\n",
    "It takes a set of <b>unlabeled</b> inputs, encodes them and then tries to extract the most valuable information from them.\n",
    "They are used for feature extraction, learning generative models of data, dimensionality reduction and can be used for compression. \n",
    "\n",
    "A 2006 paper named <b><a href=\"https://www.cs.toronto.edu/~hinton/science.pdf?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0120ENSkillsNetwork20629446-2022-01-01\">Reducing the Dimensionality of Data with Neural Networks</a>, done by G. E. Hinton and R. R. Salakhutdinov</b>, showed better results than years of refining other types of network, and was a breakthrough in the field of Neural Networks, a field that was \"stagnant\" for 10 years.\n",
    "\n",
    "Now, autoencoders, based on Restricted Boltzmann Machines, are employed in some of the largest deep learning applications. They are the building blocks of Deep Belief Networks (DBN).\n",
    "\n",
    "<center><img src=\"https://ibm.box.com/shared/static/xlkv9v7xzxhjww681dq3h1pydxcm4ktp.png\" style=\"width: 350px;\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref2\"></a>\n",
    "\n",
    "<h2>Feature Extraction and Dimensionality Reduction</h2>\n",
    "\n",
    "An example given by Nikhil Buduma in KdNuggets (<a href=\"http://www.kdnuggets.com/2015/03/deep-learning-curse-dimensionality-autoencoders.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0120ENSkillsNetwork20629446-2022-01-01\">link</a>) gives an excellent explanation of the utility of this type of Neural Network.\n",
    "\n",
    "Say that you want to extract the emotion that a person in a photograph is feeling. Take the following 256x256 pixel grayscale picture as an example:\n",
    "\n",
    "<img src=\"https://ibm.box.com/shared/static/r5knpow4bk2farlvxia71e9jp2f2u126.png\">\n",
    "\n",
    "If we just use the raw image, we have too many dimensions to analyze.  This image is 256x256 pixels, which corresponds to an input vector of 65536 dimensions! Conventional cell phones can produce images in the  4000 x 3000 pixels range, which gives us 12 million dimensions to analyze.\n",
    "\n",
    "This is particularly problematic, since the difficulty of a machine learning problem is vastly increased as more dimensions are involved. According to a 1982 study by C.J. Stone (<a href=\"http://www-personal.umich.edu/~jizhu/jizhu/wuke/Stone-AoS82.pdf?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0120ENSkillsNetwork20629446-2022-01-01\">link</a>), the time to fit a model, is optimal if:\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "<h3><strong>$$m^{-p/(2p+d)}$$</strong></h3>\n",
    "<br>\n",
    "Where:\n",
    "<br>\n",
    "m: Number of data points\n",
    "<br>\n",
    "d: Dimensionality of the data\n",
    "<br>\n",
    "p: Number of Parameters in the model\n",
    "</div>\n",
    "\n",
    "As you can see, it increases exponentially!\n",
    "\n",
    "Returning to our example, we don't need to use all of the 65,536 dimensions to classify an emotion.\n",
    "A human identifies emotions according to specific facial expressions, and some <b>key features</b>, like the shape of the mouth and eyebrows.\n",
    "\n",
    "<center><img src=\"https://ibm.box.com/shared/static/m8urvuqujkt2vt1ru1fnslzh24pv7hn4.png\" height=\"256\" width=\"256\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref3\"></a>\n",
    "\n",
    "<h2>Autoencoder Structure</h2>\n",
    "\n",
    "<img src=\"https://ibm.box.com/shared/static/no7omt2jhqvv7uuls7ihnzikyl9ysnfp.png\" style=\"width: 400px;\">\n",
    "\n",
    "An autoencoder can be divided in two parts, the <b>encoder</b> and the <b>decoder</b>.\n",
    "\n",
    "The encoder needs to compress the representation of an input. In this case, we are going to reduce the dimensions of the image of the example face from 2000 dimensions to only 30 dimensions.  We will acomplish this by running the data through the layers of our encoder.\n",
    "\n",
    "The decoder works like encoder network in reverse. It works to recreate the input as closely as possible.  The training procedure produces at the center of the network a compressed, low dimensional representation that can be decoded to obtain the higher dimensional representation with minimal loss of information between the input and the output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref4\"></a>\n",
    "\n",
    "<h2>Performance</h2>\n",
    "\n",
    "After training has been completed, you can use the encoded data as a reliable low dimensional representation of the data.  This can be applied to many problems where dimensionality reduction seems appropriate.\n",
    "\n",
    "<img src=\"https://ibm.box.com/shared/static/yt3xyon4g2jyw1w9qup1mvx7cgh28l64.png\">\n",
    "\n",
    "This image was extracted from the G. E. Hinton and R. R. Salakhutdinovcomparing's <a href=\"https://www.cs.toronto.edu/~hinton/science.pdf?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0120ENSkillsNetwork20629446-2022-01-01\">paper</a>, on the two-dimensional reduction for 500 digits of the MNIST, with PCA (Principal Component Analysis) on the left and autoencoder on the right. We can see that the autoencoder provided us with a better separation of data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref5\"></a>\n",
    "\n",
    "<h2>Training: Loss function</h2>\n",
    "\n",
    "An autoencoder uses the <b>Loss</b> function to properly train the network. The Loss function will calculate the differences between our output and the expected results. After that, we can minimize this error with gradient descent. There are many types of Loss functions, and it is important to consider the type of problem (classification, regression, etc.) when choosing this funtion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Binary Values:</h3>\n",
    "$$L(W) = - \\sum_{k} (x_k log(\\hat{x}_k) + (1 - x_k) \\log (1 - \\hat{x}_k) \\ )$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For binary values, we can use an equation based on the sum of Bernoulli's cross-entropy.  This loss function is best for binary classification problems.\n",
    "\n",
    "$x_k$ is one of our inputs and $\\hat{x}\\_k$ is the respective output.  Note that:\n",
    "\n",
    "$$\\hat{x} = f(x,W)$$\n",
    "\n",
    "where $W$ is the full parameter set of the neural network.\n",
    "\n",
    "We use this function so that when $x_k=1$, we want the calculated value of $\\hat{x}\\_k$ to be very close to one, and likewise if $x_k=0$.\n",
    "\n",
    "If the value is one, we just need to calculate the first part of the formula, that is, $-x_k log(\\hat{x}\\_k)$. Which, turns out to just calculate $- log(\\hat{x}\\_k)$.  We explicitly exclude the second term to avoid numerical difficulties when computing the logarithm of very small numbers.\n",
    "\n",
    "Likewise, if the value is zero, we need to calculate just the second part, $(1 - x_k) \\log (1 - \\hat{x}\\_k))$ - which turns out to be $log (1 - \\hat{x}\\_k) $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Real values:</h3>\n",
    "$$L(W) = - \\frac{1}{2}\\sum_{k} (\\hat{x}_k- x_k \\ )^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For data where the value (not category) is important to reproduce, we can use the sum of squared errors (SSE) for our Loss function. This function is usually used in regressions.\n",
    "\n",
    "As it was with the above example, $x_k$ is one of our inputs and $\\hat{x}\\_k$ is the respective output, and we want to make our output as similar as possible to our input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Computing Gradient</h3>\n",
    "\n",
    "The gradient of the loss function is an important and complex function.  It is defined as:\n",
    "$$\\nabla\\_{W} L(W)\\_j = \\frac{\\partial f(x,W)}{\\partial{W_j}}$$\n",
    "\n",
    "Fortunately for us, TensorFlow computes these complex functions automatically when we define our functions that are used to compute loss!  They automatically manage the backpropagation algorithm, which is an efficient way of computing the gradients in complex neural networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref6\"></a>\n",
    "\n",
    "<h2>Code</h2>\n",
    "\n",
    "We are going to use the MNIST dataset for our example.\n",
    "The following code was created by Aymeric Damien. You can find some of his code in <a href=\"https://github.com/aymericdamien?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0120ENSkillsNetwork20629446-2022-01-01\">here</a>. We made some modifications which allow us to import the datasets to Jupyter Notebooks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call our imports and make the MNIST data available to use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (1.14.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow) (1.0.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow) (0.8.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow) (3.20.0rc2)\n",
      "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow) (1.14.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow) (1.45.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: gast>=0.2.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.14.5 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow) (1.21.6)\n",
      "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow) (1.14.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow) (1.0.8)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow) (1.14.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: h5py in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from keras-applications>=1.0.6->tensorflow) (2.8.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow) (3.3.6)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow) (62.1.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow) (4.11.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow) (4.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow) (3.8.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "please upgrade to TensorFlow 2.2.0-rc0, or restart your Kernel (Kernel->Restart & Clear Output)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_70/2576421763.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'2.2.0-rc0'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'please upgrade to TensorFlow 2.2.0-rc0, or restart your Kernel (Kernel->Restart & Clear Output)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: please upgrade to TensorFlow 2.2.0-rc0, or restart your Kernel (Kernel->Restart & Clear Output)"
     ]
    }
   ],
   "source": [
    "#from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "if not tf.__version__ == '2.2.0-rc0':\n",
    "    print(tf.__version__)\n",
    "    raise ValueError('please upgrade to TensorFlow 2.2.0-rc0, or restart your Kernel (Kernel->Restart & Clear Output)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Import MINST data\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.astype('float32') / 255.\n",
    "y_test = y_test.astype('float32') / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_image_train = tf.reshape(x_train, [-1,28,28,1])  \n",
    "x_image_train = tf.cast(x_image_train, 'float32') \n",
    "\n",
    "x_image_test = tf.reshape(x_test, [-1,28,28,1]) \n",
    "x_image_test = tf.cast(x_image_test, 'float32') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the tf.keras.layers.Flatten() function to prepare the training data to be compatible with the encoding and decoding layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_layer = tf.keras.layers.Flatten()\n",
    "x_train = flatten_layer(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the <code>x_train.shape</code>  changes from (60000,28,28) to (60000, 784)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's give the parameters that are going to be used by our NN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'Tensor' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_70/2913017816.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mexamples_to_show\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtotal_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Network Parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'Tensor' has no len()"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "training_epochs = 20\n",
    "batch_size = 256\n",
    "display_step = 1\n",
    "examples_to_show = 10\n",
    "global_step = tf.Variable(0)\n",
    "total_batch = int(len(x_train) / batch_size)\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer num features\n",
    "n_hidden_2 = 128 # 2nd layer num features\n",
    "encoding_layer = 32 # final encoding bottleneck features\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> encoder </h3>\n",
    "Now we need to create our encoder. For this, we are going to use tf.keras.layers.Dense with sigmoidal activation functions. Sigmoidal functions delivers great results with this type of network. This is due to having a good derivative that is well-suited to backpropagation. We can create our encoder using the sigmoidal function like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_hidden_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_70/2998227611.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menocoding_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_hidden_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mencoding_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_hidden_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mencoding_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Building the encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n_hidden_1' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "enocoding_1 = tf.keras.layers.Dense(n_hidden_1, activation=tf.nn.sigmoid)\n",
    "encoding_2 = tf.keras.layers.Dense(n_hidden_2, activation=tf.nn.sigmoid)\n",
    "encoding_final = tf.keras.layers.Dense(encoding_layer, activation=tf.nn.relu)\n",
    "\n",
    "# Building the encoder\n",
    "def encoder(x):\n",
    "    x_reshaped = flatten_layer(x)\n",
    "    # Encoder first layer with sigmoid activation #1\n",
    "    layer_1 = enocoding_1(x_reshaped)\n",
    "    # Encoder second layer with sigmoid activation #2\n",
    "    layer_2 = encoding_2(layer_1)\n",
    "    code = encoding_final(layer_2)\n",
    "    return code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> decoder </h3>\n",
    "\n",
    "You can see that the layer\\_1 in the encoder is the layer\\_2 in the decoder and vice-versa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_hidden_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_70/1527331444.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdecoding_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_hidden_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdecoding_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_hidden_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdecoding_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Building the decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n_hidden_2' is not defined"
     ]
    }
   ],
   "source": [
    "decoding_1 = tf.keras.layers.Dense(n_hidden_2, activation=tf.nn.sigmoid)\n",
    "decoding_2 = tf.keras.layers.Dense(n_hidden_1, activation=tf.nn.sigmoid)\n",
    "decoding_final = tf.keras.layers.Dense(n_input)\n",
    "# Building the decoder\n",
    "def decoder(x):\n",
    "    # Decoder first layer with sigmoid activation #1\n",
    "    layer_1 = decoding_1(x)\n",
    "    # Decoder second layer with sigmoid activation #2\n",
    "    layer_2 = decoding_2(layer_1)\n",
    "    decode = self.decoding_final(layer_2)\n",
    "    return decode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's construct our model.\n",
    "We  define a <code>cost</code> function to calculate the loss  and a <code>grad</code> function to calculate gradients that will be used in backpropagation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "\n",
    "        self.n_hidden_1 = n_hidden_1 # 1st layer num features\n",
    "        self.n_hidden_2 = n_hidden_2 # 2nd layer num features\n",
    "        self.encoding_layer = encoding_layer\n",
    "        self.n_input = n_input # MNIST data input (img shape: 28*28)\n",
    "\n",
    "        self.flatten_layer = tf.keras.layers.Flatten()\n",
    "        self.enocoding_1 = tf.keras.layers.Dense(self.n_hidden_1, activation=tf.nn.sigmoid)\n",
    "        self.encoding_2 = tf.keras.layers.Dense(self.n_hidden_2, activation=tf.nn.sigmoid)\n",
    "        self.encoding_final = tf.keras.layers.Dense(self.encoding_layer, activation=tf.nn.relu)\n",
    "        self.decoding_1 = tf.keras.layers.Dense(self.n_hidden_2, activation=tf.nn.sigmoid)\n",
    "        self.decoding_2 = tf.keras.layers.Dense(self.n_hidden_1, activation=tf.nn.sigmoid)\n",
    "        self.decoding_final = tf.keras.layers.Dense(self.n_input)\n",
    "\n",
    "\n",
    "    # Building the encoder\n",
    "    def encoder(self,x):\n",
    "        #x = self.flatten_layer(x)\n",
    "        layer_1 = self.enocoding_1(x)\n",
    "        layer_2 = self.encoding_2(layer_1)\n",
    "        code = self.encoding_final(layer_2)\n",
    "        return code\n",
    "        \n",
    "\n",
    "    # Building the decoder\n",
    "    def decoder(self, x):\n",
    "        layer_1 = self.decoding_1(x)\n",
    "        layer_2 = self.decoding_2(layer_1)\n",
    "        decode = self.decoding_final(layer_2)\n",
    "        return decode\n",
    "\n",
    "        \n",
    "    def call(self, x):\n",
    "        encoder_op  = self.encoder(x)\n",
    "        # Reconstructed Images\n",
    "        y_pred = self.decoder(encoder_op)\n",
    "        return y_pred\n",
    "        \n",
    "def cost(y_true, y_pred):\n",
    "    loss = tf.losses.mean_squared_error(y_true, y_pred)\n",
    "    cost = tf.reduce_mean(loss)\n",
    "    return cost\n",
    "\n",
    "def grad(model, inputs, targets):\n",
    "    #print('shape of inputs : ',inputs.shape)\n",
    "    #targets = flatten_layer(targets)\n",
    "    with tf.GradientTape() as tape:    \n",
    "        reconstruction = model(inputs)\n",
    "        loss_value = cost(targets, reconstruction)\n",
    "    return loss_value, tape.gradient(loss_value, model.trainable_variables),reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training we will run for 20 epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_hidden_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_70/25021963.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRMSprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_70/3528103351.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAutoEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_hidden_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_hidden_1\u001b[0m \u001b[0;31m# 1st layer num features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_hidden_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_hidden_2\u001b[0m \u001b[0;31m# 2nd layer num features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoding_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n_hidden_1' is not defined"
     ]
    }
   ],
   "source": [
    "model = AutoEncoder()\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate)\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    for i in range(total_batch):\n",
    "        x_inp = x_train[i : i + batch_size]\n",
    "        loss_value, grads, reconstruction = grad(model, x_inp, x_inp)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    # Display logs per epoch step\n",
    "    if epoch % display_step == 0:\n",
    "        print(\"Epoch:\", '%04d' % (epoch+1),\n",
    "              \"cost=\", \"{:.9f}\".format(loss_value))\n",
    "\n",
    "print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's apply encoder and decoder for our tests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_70/1087392668.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Applying encode and decode over test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mencode_decode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflatten_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_image_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mexamples_to_show\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Applying encode and decode over test set\n",
    "encode_decode = model(flatten_layer(x_image_test[:examples_to_show]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's simply visualize our graphs!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 1 into shape (28,28)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_70/711395057.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples_to_show\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_image_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencode_decode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(a, newshape, order)\u001b[0m\n\u001b[1;32m    296\u001b[0m            [5, 6]])\n\u001b[1;32m    297\u001b[0m     \"\"\"\n\u001b[0;32m--> 298\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reshape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbound\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 1 into shape (28,28)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x144 with 20 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare original images with their reconstructions\n",
    "f, a = plt.subplots(2, 10, figsize=(10, 2))\n",
    "for i in range(examples_to_show):\n",
    "    a[0][i].imshow(np.reshape(x_image_test[i], (28, 28)))\n",
    "    a[1][i].imshow(np.reshape(encode_decode[i], (28, 28)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the reconstructions were successful. It can be seen that some noise were added to the image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Want to learn more?\n",
    "\n",
    "Also, you can use **Watson Studio** to run these notebooks faster with bigger datasets.**Watson Studio** is IBMs leading cloud solution for data scientists, built by data scientists. With Jupyter notebooks, RStudio, Apache Spark and popular libraries pre-packaged in the cloud, **Watson Studio** enables data scientists to collaborate on their projects without having to install anything. Join the fast-growing community of **Watson Studio** users today with a free account at [Watson Studio](https://cocl.us/ML0120EN_DSX).This is the end of this lesson. Thank you for reading this notebook, and good luck on your studies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thanks for completing this lesson!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created by <a href=\"https://www.linkedin.com/in/franciscomagioli?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0120ENSkillsNetwork20629446-2022-01-01\">Francisco Magioli</a>, <a href=\"https://ca.linkedin.com/in/erich-natsubori-sato?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0120ENSkillsNetwork20629446-2022-01-01\">Erich Natsubori Sato</a>, <a href=\"https://ca.linkedin.com/in/saeedaghabozorgi?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0120ENSkillsNetwork20629446-2022-01-01\">Saeed Aghabozorgi</a>\n",
    "\n",
    "Updated to TF 2.X by  <a href=\"https://www.linkedin.com/in/samaya-madhavan?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0120ENSkillsNetwork20629446-2022-01-01\"> Samaya Madhavan </a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "*   [https://en.wikipedia.org/wiki/Autoencoder](https://en.wikipedia.org/wiki/Autoencoder?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0120ENSkillsNetwork20629446-2022-01-01&cm_mmc=Email_Newsletter-\\_-Developer_Ed%2BTech-\\_-WW_WW-\\_-SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-DL0120EN-SkillsNetwork-20629446&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ)\n",
    "*   [http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/](http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0120ENSkillsNetwork20629446-2022-01-01&cm_mmc=Email_Newsletter-\\_-Developer_Ed%2BTech-\\_-WW_WW-\\_-SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-DL0120EN-SkillsNetwork-20629446&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ)\n",
    "*   [http://www.slideshare.net/billlangjun/simple-introduction-to-autoencoder](http://www.slideshare.net/billlangjun/simple-introduction-to-autoencoder?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0120ENSkillsNetwork20629446-2022-01-01&cm_mmc=Email_Newsletter-\\_-Developer_Ed%2BTech-\\_-WW_WW-\\_-SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-DL0120EN-SkillsNetwork-20629446&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ)\n",
    "*   [http://www.slideshare.net/danieljohnlewis/piotr-mirowski-review-autoencoders-deep-learning-ciuuk14](http://www.slideshare.net/danieljohnlewis/piotr-mirowski-review-autoencoders-deep-learning-ciuuk14?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0120ENSkillsNetwork20629446-2022-01-01&cm_mmc=Email_Newsletter-\\_-Developer_Ed%2BTech-\\_-WW_WW-\\_-SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-DL0120EN-SkillsNetwork-20629446&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ)\n",
    "*   [https://cs.stanford.edu/\\~quocle/tutorial2.pdf](https://cs.stanford.edu/\\~quocle/tutorial2.pdf?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0120ENSkillsNetwork20629446-2022-01-01&cm_mmc=Email_Newsletter-\\_-Developer_Ed%2BTech-\\_-WW_WW-\\_-SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-DL0120EN-SkillsNetwork-20629446&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ)\n",
    "*   <https://gist.github.com/hussius/1534135a419bb0b957b9>\n",
    "*   [http://www.deeplearningbook.org/contents/autoencoders.html](http://www.deeplearningbook.org/contents/autoencoders.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0120ENSkillsNetwork20629446-2022-01-01&cm_mmc=Email_Newsletter-\\_-Developer_Ed%2BTech-\\_-WW_WW-\\_-SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-DL0120EN-SkillsNetwork-20629446&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ)\n",
    "*   [http://www.kdnuggets.com/2015/03/deep-learning-curse-dimensionality-autoencoders.html/](http://www.kdnuggets.com/2015/03/deep-learning-curse-dimensionality-autoencoders.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0120ENSkillsNetwork20629446-2022-01-01&cm_mmc=Email_Newsletter-\\_-Developer_Ed%2BTech-\\_-WW_WW-\\_-SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-DL0120EN-SkillsNetwork-20629446&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ)\n",
    "*   [https://www.youtube.com/watch?v=xTU79Zs4XKY](https://www.youtube.com/watch?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0120ENSkillsNetwork20629446-2022-01-01&v=xTU79Zs4XKY&cm_mmc=Email_Newsletter-\\_-Developer_Ed%2BTech-\\_-WW_WW-\\_-SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-DL0120EN-SkillsNetwork-20629446&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ)\n",
    "*   [http://www-personal.umich.edu/\\~jizhu/jizhu/wuke/Stone-AoS82.pdf](http://www-personal.umich.edu/\\~jizhu/jizhu/wuke/Stone-AoS82.pdf?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0120ENSkillsNetwork20629446-2022-01-01&cm_mmc=Email_Newsletter-\\_-Developer_Ed%2BTech-\\_-WW_WW-\\_-SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-DL0120EN-SkillsNetwork-20629446&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Copyright  2018 [Cognitive Class](https://cocl.us/DX0108EN_CC). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0120ENSkillsNetwork20629446-2022-01-01&cm_mmc=Email_Newsletter-\\_-Developer_Ed%2BTech-\\_-WW_WW-\\_-SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-DL0120EN-SkillsNetwork-20629446&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
